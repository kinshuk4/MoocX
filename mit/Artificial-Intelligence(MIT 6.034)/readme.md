Lecture 1 - Introduction
---------

Build models of every problem you come across.
And before building a model, representation is very important.

AI is all about algorithms enabled by constraints exposed by representations that support models targeted at Thinking, Perception, and Action.

Once you have a name for something, you get power over it. Then, it will become a skill

Approach to a problem:
Generate and Test

Simple != Trivial

Think about State diagram.

Build smarter programs.

Continous Internal Imagination Simulation.

Lecture 2 - Reasoning - Goal tree
---------

Once you have a name for something, you get power over it. Then, you can deploy it; it will become a skill.

Stages in learning:
Witness, Understanding, and skill.

Knowledge about knowledge is Power. But, how much?
What kind of knowledge?
How knowledge representated?
How it is used?

Approach to a problem:
Problem Reduction Tree/Goal Tree

When a program fails, most interesting question can be ask.

When we understand how somethings works, it's intelligence seems to vanish.

Lecture 3 - Reasoning - Rule Based Expert System
---------

Start a new task with a motivation.

Goal tree results in answering about its own behaviour.

Complexity(Behaviour) = max(Complexity(Program), Complexity(Environment))

Rule Based "Expert" System
Forward-Chaining and Backward-Chaining

Generalise your thoughts.
But, deal with specific cases.

Rules of knowledge engineering (Heurisitics)
1. Specific cases.
2. Ask about things which appear to be same, but handled differently.
3. Build the system, and see when it cracks.

Lecture 4 - Search
---------

Gold Star Ideas:
Search is about choice.
Search algorithms is not only implementated for maps. But, for any plan we make.

Lecture 5 - Search with shortest path
---------

Think every problem in terms of nodes of a graph.
Improvement of a same algorithm is sometimes better than coming up with a new algorithm.

If heuristics are wrong:
	then you can put yourself in major trouble.
		Make sure heuristics are CONSISTENT.

Lecture 6 - Search - Games, minimax, and Alpha-Beta
---------

Develop your vocabulary.
Equipped with your idea toolbox. Algorithm toolbox.

Make layer top of existing algorithms to improve it.

Always have a stare of disbelief. 

Save as much computation as you can.

Progressive deepening.

Anytime Algorithms - Always have an answer within time given to work, because it is doing progressive deepening.

Human intelligence is different from Aritificial Intelligence, but that doesn't mean it possess no intelligence at all.

Lecture 7 - Constraints
---------

Always learn the story how a thing came to be, before learning about it.

"When a particular program doesn't work?", is one of the important question one can ask.

Before devising an algorithms, one can make any number of assumptions (Constraints), granted algorithm will only work if we consider those assumptions(Constraints).

Devise your vocabulary before devising an algorithm.

Always have a powerful constraint.

You need a problem to start with; you need a method that works, because of some principles.

Working with neighbour constraints in an algorithm.

Constraint Propogation Algorithm

Lecture 8 - Constraints (Map coloring problem)
---------

Don't give up and cry.

Develop your vocabulary.

Domain Reduction Algorithm.

Most constraints first is better than least constraints first.

Map coloring problem is analogous to resource planning problem.

Always look for similiar ideas. Mostly, already made algorithms can be developed to solve that problem.

Always have upper limit, then goes back to lower limit to check the breaking point of algorithm.

Lecture 9 - Constraints (Visual Object Recognization)
---------

Things move slowly. Life moves slowly. This is how everything works.

Always develop a theory, before implementing it.

Always go for problem reduction. Ask yourself, have I seen this problem before?

Re-invent ideas.

Correlation works.

Naming the concept is also important.

Lot of unanswered question in computer vision.

Story telling goes in our mind all the time, which makes us intelligent.

Lecture 10 - Learning - Nearest Neighbour
----------

The most important thing, we human learned is to merge two concepts and create a new one.

Always start with a story.

Methods to solve Learning Problem:
Nearest Neighbour (Pattern Recognization)
Neuron Networks
Boosting

Always try the simplest thing rather than difficult one.

If something is similiar in some respect, it is likely it'll be similiar to others.

Information Retrival Problem.

One of the way we learn is to record parameters of a particular problem; multiple parameters, multiple times. And then, when real problem comes up, we lookup in the table, and find the nearest neighbour, and work according to it.

Have a nice Cocktail conversation regarding ideas.

Sometimes the problem is independent of data involved.

Types of learning in human
1. One shot
2. Explanation based learning. (EBL)

Sleep deprieved results in lack of doing calculations.

Lecture 11 - Learning - Identification Tree
----------

Simpliest explanation is often better than complicated one.

Always think of ideal case, then make your way to the current situation.

Proper data or large amount of data is important for learning.

Graphs are very important.

Always improve your current idea. Don't discard it until you're convinced.

Always have intuition before implementing it.

Derive your ideas from others' idea.

Lecture 12 - Learning - Neural Nets 
----------

Mimicking biology.

Make a model of everything.

If you symbolise everything, complex problems seems easy.

Have a good insight of a problem while approaching the solution.

Mathematical Convenience function.

What the tech actually doing?

Lecture 13 - Learning - Genetic Algorithms
----------

Eat chocolate.

Being a computer scientist, we love binary.

Life is full of choices. A lot, a LOT.

Don't conclude an idea to be bad.

Always look for trouble in your algorithm. Keep on improving it.

Ideas are more powerful than programs.

Plan making can be done using genetic algorithm.

Try to devise every problem to previous problem you have encountered. 

Problem reduction is also importatnt.

Where the credit lies? To the programmer or the creater of the algorithm.

Lecture 14 - Learning - Sparse Spaces, Phonology
----------

What if God is an engineer?

What you see has large influence on what you hear.

Always make a model.

Features is what we learn. Try to identify features.

Learning is about collecting a lot of examples.

Generalise your theory.

How to tackle an AI problem:
1. Specify the problem.
2. Devise a representation suited to the problem.
3. Determining an approach/method.
4. Pick a mechanism, or devise an algorithm.
5. Experiment.
All goes in loop

Don't pick a single mechanism. Don't fall in love with one when you have lot of other tools.

What is good representation?
1. It makes right things explicit.
2. Expose constraints.
3. Localness.

Lecture 15 - Learning - Near misses
----------

Learning about AI, makes one smarter. It is because we are understanding how we learn.

Learning in one go. Every example leads to learning a single important thing.

Pick a seed.
Apply heuristics to cover most of the positive examples.

You should name everything to get power over it.

Specialise and generalise your algorithm to make it more robust.

Examples always generalise, and near miss specialise your algorithm.

What we know in our mind, always forms a graph.

In order to learn anything, one has to make description inside one own mind. Unless, you talk to ourself.

By talking to yourself, one can score better. But, don't do it in public.

How to package your idea better?
1. Symbol
2. Slogan
3. Surprise
4. Salient
5. Story

All education is about story telling, story understanding, and story making.

AI is not about writing smarter programs, it is about making one self more smarter.

Lecture 16 - Learning - Support vector machines
----------

How ideas are made?

When struck switch to other perceptive.

Have patience; remember this. Brilliant algorithms are made in years, not in hours.

Lecture 17 - Learning - Boosting
----------

Learn to find or identify the powerful ideas.

Let things sing song to you.

Have a multi-part idea.

Wisdom of weighted crowd of experts.

Always remember the gold star ideas.

Search for 'Thank God holes'.

Lecture 18 - Representations - Classes
----------

What makes us intelligent?
Take two concepts and make a new concept out of it without limit.

Our all education is about story telling.

Inner language - language with thich we think.
1. Combinators
2. Reification
3. Localisation
4. Sequence

Representation consists of:

1. Classification

Three classification of knowledge:
1. General e.g Fruit
2. Basic (We hang use amount of knowledge) e.g Apple
3. Specific e.g Mac

2. Transitions

3. Trajectory/Roll 

We can have multiple representations. Which in turn make us smarter because we have all desired input from the world.

With proper representation or data structure, we can answer better.

We got whole lot of story libraries in our mind.

How to be a better writer?
1. Don't use pronouns.
2. Don't use former or latter.
3. Don't call a shovel a spade.

Lecture 19 - Architectures : GPS, SOAR, Subsumption, Soceity of mind
----------

General Problem Solver (GPS)
Problem Solving Hypothesis.
Consider every situation in a state diagram. And move from current state to goal state.

Always know what the scope of architecture.

State Operator And Result (SOAR)
Symbol System Hypothesis.

Emotion Machines:
Thinking in different layers.

Subsumption:
Building over other ideas are better. You can take the leaverage of their work.

No Representation.
Use world rather than a model.
Finite state machines.

Imagination is very important.

Strong Story Hypothesis.

Child becomes a adult when they start describing the world.

Gold Star Ideas
1. Look-Listen-Draw-Talk
2. Beware of fast talkers because they jam your language processor.

Lecture 21 - Probabilistic Inference I
----------

Instead of collecting all the data, just collect dependent data.

Approach the problem via imaging the solution; and trace back to the beginning.

Lecture 22 - Probabilistic Inference II
----------

Learn to do problem reduction.

Lecture 23 - Model Merging
----------

AI has three perspective:
1. Scientific (Representationg, methods, architectures)
2. Engineering (Applications)
3. Bussiness 

AI is not about replacing people, but to co-exist.

Don't be mechanism envy. Try to use all tools in toolbox.

Powerful Ideas
--------------

The idea of powerful idea.
The right representations make you smarter.
Sleep makes you smarter.
You cannot learn unless you almost know.
We think with mouths, eyes, and hands.
The Strong Story Hypothesis.
All great ideas are simple.

help from:
 - https://github.com/reetawwsum/Machine-learning-MOOC-notes/blob/master/Artificial%20Intelligence%20(MIT%206.034).txt