---
title: "Analytic Edge Kaggle NYT classifiction"
author: "Ryan Zhang"
date: "Thursday, April 16, 2015"
output:
  html_document:
    fig_height: 4
    fig_width: 5
    highlight: espresso
    keep_md: yes
    theme: cosmo
    toc: yes
---

#0 Environment      
##0-1 Set working directory  �     
```{r }
setwd("~/GitHub/edX/The Analytic Edge/Kaggle")
```

##0-2 Load Libraries   �  
```{r warning=FALSE,message=FALSE}
library(tm)
library(e1071)
library(neuralnet)
library(randomForest)
library(ROCR)
library(party)
library(rCUR)
library(topicmodels)
library(xgboost)
```

##0-3 Function Definition  �Զ��庯��  
 ڰ table �ٷֱȵ�С �    
```{r}
tCor <- function(t)round(t[,2]/rowSums(t),2)*100 
```

  dummy encoding ݿ�
```{r}
dummyEncoding <- function(df, colname){
  dDF <- as.data.frame(model.matrix(~df[,colname]))
  names(dDF) <- paste(colname,as.character(levels(df[,colname])),sep="")
  dDF}
```

#1 Data Preparing  �׼ �
##1-1 Loading װ��
```{r}
NewsTrain <- read.csv("NYTimesBlogTrain.csv", stringsAsFactors = F)
NewsTest <- read.csv("NYTimesBlogTest.csv", stringsAsFactors = F)
```

##1-2 Ԥ �
Store the number of training data points and the number of testing data points.        
��¼һ��ѵ ݺͲ �ݵ ��
```{r}
ntrain <- nrow(NewsTrain)
ntest <- nrow(NewsTest)
ntrain
ntest
```

"Popular"" is the dependant variable, store it in a separate vector "Y", and delete the colomn from the dataframe "NewsTrain".      
ҪԤ  �ǡ�Popular  �һ  "Y" �, �ѵ ݿ ɾ С�
```{r}
Y <- as.factor(NewsTrain$Popular)
NewsTrain$Popular <- NULL
```

Combine "NewsTrain" and "NewsTest" into a single dataframe for the purpose of data preparing      
��ѵ ݺͲ �ݺϲ�Ϊһ �һ ݿ��Ա㼯�д �Ƿ �⣿��    
**ֻҪ�ǷǼල�ı任Ӧ�ö  �**
```{r}
OriginalDF <- rbind(NewsTrain, NewsTest)
```

Filling empty entries for the first three columns with name "Other"      
��ǰ  ġ �á�Other 
```{r}
for (i in 1:nrow(OriginalDF)){
  for (j in 1:3){
    if (OriginalDF[i,j] == ""){
      OriginalDF[i,j] <- "Other"}}}
```

Change the first three columns to be factors     
��ǰ  �ĳ�factor �
```{r}
OriginalDF$NewsDesk <- as.factor(OriginalDF$NewsDesk)
OriginalDF$SectionName <- as.factor(OriginalDF$SectionName)
OriginalDF$SubsectionName <- as.factor(OriginalDF$SubsectionName)
```

Log Transform "WordCount"      
��WordCount �ת �ת Ϊ��̬�ֲ   
```{r}
# OriginalDF$ZWordCount <- with(OriginalDF, (WordCount - mean(WordCount))/sd(WordCount))
OriginalDF$NWordCount <- log(OriginalDF$WordCount + 1)
```

Conver the PubDate and time variable to be more R friendly and extract the hour of day, the day on month and the day of week to be seperate variables. Finally delete the PubDate column.       
��PubDate�ĳ�R �-ʱ ʽ ��ܼ ÿ�¼ �Լ�ÿ�켸 �Щ��Ϣ �ȡ �ɾ��ԭ �PubDate
```{r}
OriginalDF$PubDate <- strptime(OriginalDF$PubDate, "%Y-%m-%d %H:%M:%S")
OriginalDF$Hour <- as.factor(OriginalDF$PubDate$h)
OriginalDF$Wday <- as.factor(OriginalDF$PubDate$wday)
OriginalDF$Mday <- as.factor(OriginalDF$PubDate$mday)
# OriginalDF$isWeekend <- as.numeric(OriginalDF$Wday %in% c(0,6))
OriginalDF$PubDate <- NULL
```

Generate training and testing set    
 �ѵ Ͳ ��    
```{r}
train <- OriginalDF[1:ntrain, c(1:3,9:12)]
test <- OriginalDF[(ntrain+1):nrow(OriginalDF),c(1:3,9:12)]
```

##2 Exploratory Data Analysis  ̽��ʽ ݷ     
First Explore the few factor variable and their relationship to the depandent variable.    
�ȿ ǰ �factor  ҪԤ Popular֮��Ĺ�ϵ��    
```{r}
tNewsDesk <- table(OriginalDF$NewsDesk[1:ntrain], Y)
t(tNewsDesk)
tCor(tNewsDesk)
plot(tCor(tNewsDesk))

tSectionName <- table(OriginalDF$SectionName[1:ntrain], Y)
t(tSectionName)
tCor(tSectionName)
plot(tCor(tSectionName))

tSubsectionName <- table(OriginalDF$SubsectionName[1:ntrain], Y)
t(tSubsectionName)
tCor(tSubsectionName)
plot(tCor(tSubsectionName))
```
    
SectionName, SubsectionName,NewsDeskӦ�ö Ԥ ��ı Ӧ�ñ    

Looking at the text contents    
 ı Ϣ   
It seems that the "Snippet" is almost redudent with "Abstract", in since 98% cases they are the same. And "Abstract" contains a little bit more infomation than "Snippet"      
SnippetӦ�ú�Abstract غ �ݷǳ��࣬ǰ��òT�ƶ �ں��ߣ  ֻ�ú��߾ͺ��ˡ�    
```{r}
sum(OriginalDF$Snippet == OriginalDF$Abstract)/nrow(OriginalDF)
which(OriginalDF$Snippet != OriginalDF$Abstract)[1]
OriginalDF[22,5]
OriginalDF[22,6]
```

Looking at WordCount    
      
The distribution of WordCount seems to be a longtail / power-law distribution.    
 ķֲ��ƺ �ɷֲ     
```{r}
summary(OriginalDF$WordCount)
hist(OriginalDF$WordCount, breaks = 70)
hist(OriginalDF$NWordCount)
```

Looking at publication day/weekday/hour related to Popular   
 �Сʱ ܼ ÿ�¼��ţ Щ��û �   
```{r}
tHour <- table(OriginalDF$Hour[1:ntrain] , Y)
t(tCor(tHour))
plot(tCor(tHour))

tWday <- table(OriginalDF$Wday[1:ntrain], Y)
t(tCor(tWday))
plot(tCor(tWday))

tMday <- table(OriginalDF$Mday[1:ntrain], Y)
t(tCor(tMday))
plot(tCor(tMday))

#tWeekend <- table(OriginalDF$isWeekend[1:ntrain], Y)
#tCor(tWeekend)
#plot(tCor(tWeekend))
```
ÿ�¼��ſ ȥûɶ��   

#3 Model fitting  ģ   
##3-1 randomForest on Non-Text Features �ò �ı ȡ  �ɭ��
randomForest model    
 ɭ��ģ��    
public 0.92420
```{r  cache=TRUE}
set.seed(1126)
rfModel <- randomForest(x = train,
                        y = Y,
                        ntree = 1000,
                        mtry = 2,
                        nodesize = 4,
                        importance = F,
                        proximity = F)
```

Evaluate on Training set 
 ݲ��Լ ģ��
```{r cache=TRUE}
rfPred <- predict(rfModel, train, type = "prob")
table(rfPred[,2] > 0.5,Y)

prediction <- ROCR::prediction(rfPred[,2], Y)
perf <- performance(prediction, "tpr", "fpr")
plot(perf, colorize = T, lwd = 2)
auc <- performance(prediction, "auc")
auc@y.values
```

Make prediction with randomForest model
 ɭ��ģ �Ԥ��
```{r }
tpred <- predict(rfModel, test, type = "prob")
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = tpred[,1])
write.csv(MySubmission, "rfRegularFeatures2.csv", row.names = F)
```

#4 Try feature engineering with text content  �ͨ ı        
##4-1 TFIDF    �Ƶ�Ρ �ĵ�Ƶ��  
Extract all headline and abstract to form a corpus    
��ȡ �ժҪ�ı һ Ͽ�    
```{r cache=TRUE}
text <- vector()
for (i in 1:nrow(OriginalDF)) {
  text <- rbind(text, paste(OriginalDF$Headline[i], " ", OriginalDF$Abstract[i]))
}

Corpus <- Corpus(VectorSource(text))
```

Standard Corpus processing     
��׼ ��Ͽ⴦��     
```{r cache=TRUE}
Corpus <- tm_map(Corpus, tolower)     
Corpus <- tm_map(Corpus, PlainTextDocument)    
Corpus <- tm_map(Corpus, removePunctuation)    
Corpus <- tm_map(Corpus, removeWords, stopwords("english"))     
Corpus <- tm_map(Corpus, stemDocument)
```

��Ϊ Ƴ�һЩ �
```{r}
Corpus <- tm_map(Corpus, removeWords, c("new","time","york","today","day","said","say","report","week","will","year","articl","can","daili","news"))
```

Document ~ TF-IDF matrix    
 ĵ�~TFIDF �     
```{r cache=TRUE}
dtm <- DocumentTermMatrix(Corpus, control = list(weighting = weightTfIdf))   
tfdtm <- DocumentTermMatrix(Corpus)   
```

 �Щ�ʱȽ�Ƶ��
```{r}
sort(colSums(as.matrix(dtm)),decreasing = T)[1:20]
```


Get the terms    
��ȡ б�     
```{r }
terms <- dtm$dimnames$Terms    
terms[5101:5110]
```

Get the matrix for training and testing set     
�ֱ ѵ Ͳ �ݵ�Document~TF-IDF �     
```{r }
dtmTrain <- dtm[1:ntrain,]
dtmTest <- dtm[(1+ntrain):dtm$nrow,]
```

Get frequent terms matrix for testing set
��ò��Լ Ƶ �
```{r }
sparseTest <- removeSparseTerms(dtmTest, 0.97)
wordsTest <- as.data.frame(as.matrix(sparseTest))
termsTest <- names(wordsTest)
```

Filter the dtm based on frequent terms in testing set    
 ݲ��Լ Ƶ ��ԭ ľ �ɸѡ     
```{r cache=TRUE}
cols <- vector()
for (i in 1:length(termsTest)){
  cols = c(cols, which((terms == termsTest[i]) == T))}
dtmFiltered <- dtm[,cols]
```

Text Feature    
�ı       
```{r cache=TRUE}
termFeatures <- as.data.frame(as.matrix(dtmFiltered))
row.names(termFeatures) <- c(1:nrow(OriginalDF))
```

Append text features to the dataframe    
```{r}
TextADDDF <- as.data.frame(termFeatures)
```

```{r}
tatrain <- cbind(train, TextADDDF[1:ntrain,])
tatest <- cbind(test, TextADDDF[(ntrain+1):nrow(TextADDDF),])
```

##4-2 randomForest model with text features added   ı  ɭ��ģ��  
public  0.92420
```{r cache=TRUE}
set.seed(1126)
tarfModel <- randomForest(x = tatrain,
                        y = Y,
                        ntree = 1000,
                        nodesize = 4,
                        importance = T,
                        proximity = F)
```

Look at the importance of features via training randomForest   
 ı  �Ҫ��    
```{r}
t(sort(tarfModel$importance[,4],decreasing = T))
```
  
Make prediction on the training set
�ü �ı  ɭ��ģ�Ͷ�ѵ ݽ Ԥ��
```{r cache=TRUE}
tarfPred <- predict(tarfModel, tatrain, type = "prob")
table(tarfPred[,2] > 0.5,Y)

prediction <- ROCR::prediction(tarfPred[,2], Y)
perf <- performance(prediction, "tpr", "fpr")
plot(perf, colorize = T, lwd = 2)
auc <- performance(prediction, "auc")
auc@y.values
```

Make prediction with randomForest model
 ı  ɭ��ģ �Ԥ��
```{r}
tatpred <- predict(tarfModel, tatest, type = "prob")
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = tatpred[,1])
write.csv(MySubmission, "rfText2.csv", row.names = F)
```

##4-3 Why not a neural net?          
Neural net based on numerical features.    
 �ֵ  �磨û��GPU          
```{r cache=TRUE}
# nntrain <- tatrain[,c(4,8:40)]
# nntest <- tatest[,c(4,8:40)]
# nntrain$Popular <- as.numeric(as.character(Y))
# nameofvars <- names(nntrain)
# nnformula <- as.formula(paste("Popular ~", 
#                         paste(nameofvars[!nameofvars %in% "Popular"], collapse = " + ")))
# ptm <- proc.time()
# nn <- neuralnet(formula = nnformula,
#                 data = nntrain, 
#                 hidden = 2,
#                 threshold = 0.02,
#                 stepmax = 1e8,
#                 err.fct="ce",
#                 linear.output=FALSE)
# plot(nn)
# pnn <- compute(nn,nntrain[,1:34])
# summary(pnn$net.result)
# nnpredict <- as.vector(pnn$net.result)
# prediction <- ROCR::prediction(nnpredict, Y)
# perf <- performance(prediction, "tpr", "fpr")
# plot(perf, colorize = T, lwd = 2)
# auc <- performance(prediction, "auc")
# auc@y.values
# proc.time() - ptm
```

##4-4 Support Vector Machine ֧  
 �Dummy Encoding
```{r cache=TRUE}
taDF <- rbind(tatrain, tatest)
names(taDF)
tadDF <- taDF[,c(4,8:39)]
toEnco <- c("NewsDesk","SectionName","SubsectionName","Hour","Wday","Mday")
for (colname in toEnco){
  tadDF <- cbind(tadDF, dummyEncoding(taDF, colname))
}
names(tadDF) <- make.names(tadDF)
tadtrain <- tadDF[1:ntrain,]
tadtest <- tadDF[((1+ntrain):nrow(tadDF)),]
```

SVM Model Fitting ֧  
public  0.88249
```{r cache=TRUE,warning=FALSE}
svmModel <- svm(x = tadtrain, 
                y = Y,
                cost = 100, gamma = 1, probability = T)
svmpred <- predict(svmModel, tadtrain, decision.values = T, probability = T)
svmpredp <- attr(svmpred, "probabilities")[,1]
prediction <- ROCR::prediction(svmpredp, Y)
perf <- performance(prediction, "tpr", "fpr")
plot(perf, colorize = T, lwd = 2)
auc <- performance(prediction, "auc")
auc@y.values
svmtpred <- predict(svmModel, tadtest, probability = T)
svmtpredp <- attr(svmtpred, "probabilities")[,1]
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = svmtpredp)
write.csv(MySubmission, "svmText2.csv", row.names = F)
```

##4-5 Simple Ensemble �򵥵 �ģ��
```{r cache=TRUE}
enpred <- 0.2*svmtpredp+0.8*tatpred[,2]
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = enpred)
write.csv(MySubmission, "svm_rf_ensemble2.csv", row.names = F)
```


#5 Clustering on TFIDF matrix  ��࿴��
```{r cache=TRUE}
cTest <- removeSparseTerms(dtmTest, 0.98)
cWords <- as.data.frame(as.matrix(cTest))
cTerms <- names(cWords)

cols <- vector()
for (i in 1:length(cTerms)){
  cols = c(cols, which((terms == cTerms[i]) == T))
}
cdtm <- dtm[,cols]
cMatrix <- as.matrix(cdtm)
cDF <- as.data.frame(cMatrix)

kmclusters <- kmeans(cDF, 9, iter.max = 5000)

tactrain <- cbind(tatrain, kmclusters$cluster[1:ntrain])
names(tactrain) <- c(names(tactrain)[1:39],"cluster")
tactest <- cbind(tatest, kmclusters$cluster[(1+ntrain):nrow(OriginalDF)])
names(tactest) <- c(names(tactest)[1:39],"cluster")
```

##5-1 Another randomForest with cluster labels added  Ͼ �ǩ �һ ɭ�� 

```{r cache=TRUE}
set.seed(1126)
tacrfModel <- randomForest(x = tactrain,
                        y = Y,
                        ntree = 1000,
                        mtry = 6,
                        nodesize = 4,
                        importance = T,
                        proximity = F)
t(sort(tacrfModel$importance[,4],decreasing = T))
tacrfPred <- predict(tacrfModel, tactrain, type = "prob")
table(tacrfPred[,2] > 0.5,Y)

prediction <- ROCR::prediction(tacrfPred[,2], Y)
perf <- performance(prediction, "tpr", "fpr")
plot(perf, colorize = T, lwd = 2)
auc <- performance(prediction, "auc")
auc@y.values

tacrfPred <- predict(tacrfModel, newdata = tactest, type = "prob")
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = tacrfPred[,2])
write.csv(MySubmission, "rfTextCluster2.csv", row.names = F)
```

#6 Matrix Factorization  �ֽ�  
##6-1 SVD  �ֵ�ֽ�
```{r cache=TRUE}
s <- svd(cMatrix)
Sig <- diag(s$d)
plot(s$d)
totaleng <- sum(Sig^2)
engsum <- 0
for (i in 1:nrow(Sig)){
  engsum <- engsum + Sig[i,i]^2
  if (engsum/totaleng > 0.8){
    print(i)
    break}}
```
��Ҫ51  ܱ ԭTF-IDF �80% �   

##6-2 CUR Matrix Decomposition CUR �ֽ�
```{r cache=TRUE}
res <- CUR(cMatrix, c = 9, r = 85, k = 51)
```

��ͶӰ�ӵ�ѵ  ��
```{r cache=TRUE}
ncolC <- ncol(getC(res))
Ak <- getC(res) %*% getU(res)[,1:ncolC]
AkDF <- as.data.frame(Ak)
tacftrain <- cbind(tactrain, AkDF[1:ntrain,])
tacftest <- cbind(tactest, AkDF[((1+ntrain):nrow(AkDF)),])
```

##6-3 randomForest with pc added  �Ҫ�ɷ�ͶӰ �ɭ��

```{r cache=TRUE}
set.seed(1126)
tacfrfModel <- randomForest(x = tacftrain,
                        y = Y,
                        ntree = 1000,
                        mtry = 8,
                        nodesize = 4,
                        importance = T,
                        proximity = F)
t(tacfrfModel$importance[,4])
tacfrfPred <- predict(tacfrfModel, tacftrain, type = "prob")
table(tacfrfPred[,2] > 0.5,Y)

prediction <- ROCR::prediction(tacfrfPred[,2], Y)
perf <- performance(prediction, "tpr", "fpr")
plot(perf, colorize = T, lwd = 2)
auc <- performance(prediction, "auc")
auc@y.values

tacfrfPred <- predict(tacfrfModel, newdata = tacftest, type = "prob")
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = tacfrfPred[,2])
write.csv(MySubmission, "rfTextClusterFactorization2.csv", row.names = F)
```

#7 LDA?
public 0.92507
```{r}
lda <- LDA(tfdtm, 11)
set.seed(1126)
tacfltrain <- cbind(tacftrain,topics(lda)[1:ntrain])
tacfltest <- cbind(tacftest, topics(lda)[(1+ntrain):nrow(OriginalDF)])
names(tacfltrain) <- c(names(tacfltrain)[1:49],"LDA")
names(tacfltest) <- c(names(tacfltest)[1:49],"LDA")
tacflrfModel <- randomForest(x = tacfltrain,
                        y = Y,
                        ntree = 1000,
                        mtry = 8,
                        nodesize = 4,
                        importance = T,
                        proximity = F)
t(sort(tacflrfModel$importance[,4],decreasing = T ))
tacflrfPred <- predict(tacflrfModel, tacfltrain, type = "prob")
table(tacflrfPred[,2] > 0.5,Y)

prediction <- ROCR::prediction(tacflrfPred[,2], Y)
perf <- performance(prediction, "tpr", "fpr")
plot(perf, colorize = T, lwd = 2)
auc <- performance(prediction, "auc")
auc@y.values

tacflrfPred <- predict(tacflrfModel, newdata = tacfltest, type = "prob")
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = tacfrfPred[,2])
write.csv(MySubmission, "rfTextClusterFactorizationLDA2", row.names = F)
```

