{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "First let's have a look at what convolutions do\n",
    "\n",
    "### Learning Activity 1: Load the Python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by loading the necessary Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import misc\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 2: Import the data\n",
    "\n",
    "We are going to start by loading the portrait of Grace Hopper using the `imread()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the grace_hopper.jpg image from the data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flag flatten means you imported the image as a grey-scale image. This means each pixel is represented by a single value between 0 (black) and 255 (white). You can view the image using the `imshow` function from `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or view a part of the image by selecting a region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View a region of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can view the values of the pixels directly, for example to view the values in the top left corner of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the pixel values of a region in the top left corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 3: Define and apply a convolution function\n",
    "\n",
    "Now lets define a convolution function. First you must define a function which traverses the image to apply the convolution at every point and returns the result in a filtered image. Calculating the size of the filtered image along each dimension can be a little tricky, the formula is: \n",
    "\n",
    "                         Size of the filtered image = input image size - filter size + 1\n",
    "\n",
    "Let us start by implementing the `convolve` function. It takes as input an image and a filter, and returns\n",
    "the output of applying the filter at each position in the image through a function `multiply_sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution function\n",
    "\n",
    "def convolve(image, filter):\n",
    "    filter_height = filter.shape[0]\n",
    "    filter_width  = filter.shape[1]\n",
    "    filtered_image = np.ndarray(shape=(image.shape[0] - filter_height + 1, \n",
    "                                       image.shape[1] - filter_width + 1))\n",
    "    \n",
    "    for x in range(0, filtered_image.shape[0]):\n",
    "        for y in range(0, filtered_image.shape[1]):\n",
    "            # We select a local patch of the image\n",
    "            patch = image[x: x + filter_height, \n",
    "                          y: y + filter_width]\n",
    "            \n",
    "            # Then apply the convolution operation to it\n",
    "            filtered_image[x,y] = multiply_sum(patch, filter)\n",
    "    return filtered_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the `multiply_sum` function. It takes as input two `numpy` arrays of the same shape, multiplies them elementwise and returns the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiply_sum function\n",
    "\n",
    "def multiply_sum(patch, filter):\n",
    "    # Let's make sure our two inputs have the same shape\n",
    "    assert(patch.shape == filter.shape)\n",
    "    return np.sum(patch * filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An there you have it, a convolution operator! You can apply a filter onto an image and see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#First define the 3x3 filter \n",
    "\n",
    "#Then apply it onto the image \n",
    " \n",
    "#Show the result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: What did our filter do?** \n",
    "\n",
    "1) By looking at the image, can you tell what kind of pattern the filter detected?\n",
    "\n",
    "2) How would you design a filter which detects vertical edges?\n",
    "\n",
    "3) What would the following filter do:\n",
    "\n",
    "filter = np.array([[ 1,  1,  1], \n",
    "                   [ 0,  0,  0], \n",
    "                   [-1, -1, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convolve using this filter \n",
    "\n",
    "# Show the result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose the filter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convolve using this filter \n",
    "\n",
    "# Show the result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 4: Convolutions with colour\n",
    "\n",
    "Very good! But what if we had a colour image, how would we use that extra information to detect useful patterns? The idea is simple, on top of having a set weight for each pixel, we have a set weight for each colour channel within that pixel. The following kernel detects region of the image which are mostly brown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a brown filter \n",
    "\n",
    "brown_filter = np.array(\n",
    "      [[[ 0.13871045,  0.17157242,  0.12934428],\n",
    "        [ 0.16168842,  0.20229845,  0.14835016],\n",
    "        [ 0.135694  ,  0.16206263,  0.11727387]],\n",
    "\n",
    "       [[ 0.04231958,  0.05471011,  0.03167877],\n",
    "        [ 0.0462575 ,  0.06581022,  0.03104937],\n",
    "        [ 0.04185439,  0.04734124,  0.02087744]],\n",
    "\n",
    "       [[-0.15704881, -0.16666673, -0.16600266],\n",
    "        [-0.17439997, -0.17757156, -0.18760149],\n",
    "        [-0.15435153, -0.17037505, -0.17269668]]])\n",
    "\n",
    "print(brown_filter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension represents colours. You can see the filter responds to regions that are red, a little green, and not at all blue. Let's load a colourful version of the Grace Hopper portrait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read and show the Grace Hopper portrait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz** \n",
    "\n",
    "Can you design a filter which will detect the edge from the background (blue) to Grace Hopper’s left shoulder (black).\n",
    "\n",
    "*Hint*: make sure the weights in your lter sum to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We could almost apply our convolution operator already, but the filter we have is currently in the wrong format. The colour channel dimension should be the last one of the filter, same as in our image. This can be fixed by a simple transpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Transpose the brown filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load an already trained network in our environment. This network (VGG-16) has been trained on the Imagenet dataset where the goal is to classify pictures into one of one thousand categories. When it came out in 2014, it won the annual ImageNet Recognition Challenge correctly classifying 93% of the images in the test set. For comparison, humans can achieve around 95% accuracy. It's also very simple, it only uses 3x3 convolutions! It is very deep though and it takes 4 GPUS 2-3 weeks to train it.\n",
    "\n",
    "To load the model, you must first define it's architecture. You're going to do this step by step as you learn the components of convolutional neural networks. But first, lets load the necessary libraries. We are again going to use the `Keras` library.\n",
    "\n",
    "### Learning Activity 5: Load the Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 6: Implementing a convolutional layer\n",
    "\n",
    "You are going to define the first convolutional layer of the network. But before, you will add some padding to the image so the convolutions get to apply on the outer edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement a convolutional layer\n",
    "\n",
    "# Create the model\n",
    "# On the very first layer, you must specify the input shape\n",
    "# Your first convolutional layer will have 64 3x3 filters, and will use a relu activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 7: Stacking layers\n",
    "\n",
    "Now you're going to stack another convolutional layer. Remember, the output of a convolutional layer is a 3-D tensor, just like our input image. Although it does have a much higher depth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stacking layers \n",
    "# Once again you must add padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Learning Activity 8: Adding pooling layers\n",
    "\n",
    "Now lets add your first pooling layer. Pooling reduces the width and height of the input by aggregating adjacent cells together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a pooling layer with window size 2x2\n",
    "# The stride indicates the distance between each pooled window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 9: Adding more convolutions for VGG\n",
    "\n",
    "Now you can stack many more of these! Remember not to change the parameters as we are about to load the weights of an already trained version of this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lots more Convolutional and Pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the depth of the layers get progressively larger, up to 512 for the latest layers. This\n",
    "means as we go along, each layer detects a greater number of features. On the other hand, each\n",
    "max-pooling layer halves the height and width of the layer outputs. Starting from images of dimensions\n",
    "224x224, the final outputs are only of size 7x7.\n",
    "\n",
    "Now you're about to add some fully connected layers which can learn the more abstract features of the image. But first you must first change the layout of the input so it looks like a 1-D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Flatten the input\n",
    "\n",
    "# Add a fully connected layer with 4096 neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Flatten` function removes the spatial dimensions of the layer output, it is now a simple 1-D row of numbers. This means we can no longer apply convolutions, but can apply fully connected layers like the ones of the perceptron from the previous module.\n",
    "\n",
    "`Dense` layers are fully connected layers. You used them in the previous module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 10: Preventing overfitting with Dropout\n",
    "\n",
    "`Dropout` is a method used at train time to prevent overfitting. As a layer, it randomly modifies its input\n",
    "so that the neural network learns to be robust to these changes. Although you won’t actually use it\n",
    "now, you must define it to correctly load the weights as it was part of the original network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a dropout layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number 0.5 indicates the amount of change, 0.0 means no change, and 1.0 means completely different.\n",
    "\n",
    "\n",
    "Add one more fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add another fully connected layer with 4096 neurons and a Dropout at the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally a softmax layer to predict the categories. There are 1000 categories and hence 1000 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add softmax layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 11: Loading the weights\n",
    "\n",
    "And you're all set! Let's load the weights of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the weights \n",
    "\n",
    "# Compile the network no need to worry about this for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 12: Preprocessing the data\n",
    "\n",
    "Lets feed an image to your model. In the VGG network, we only do zero centering. The model takes as input a slightly transformed version of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the image\n",
    "\n",
    "img = cv2.resize(cv2.imread('data/cat.jpg'), (224, 224))\n",
    "\n",
    "# Transform it to the right formatd\n",
    "def transform_image(image):\n",
    "    image_t = np.copy(image).astype(np.float32)       # Avoids modifying the original\n",
    "    image_t[:,:,0] -= 103.939                    # Substracts mean Red\n",
    "    image_t[:,:,1] -= 116.779                    # Substracts mean Green\n",
    "    image_t[:,:,2] -= 123.68                     # Substracts mean Blue\n",
    "    image_t = image_t.transpose((2,0,1))         # The colour dimension comes first\n",
    "    image_t = np.expand_dims(image_t, axis=0)    # The network takes batches of images as input\n",
    "    return image_t\n",
    "\n",
    "img_t = transform_image(img)\n",
    "\n",
    "# What does the image look like?\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 13: Getting an output from the network\n",
    "\n",
    "Now push it through the network and get the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Push the image through\n",
    "\n",
    "#The network takes batches of images, we only want the result for one image                   \n",
    "\n",
    "#The output is an array with 1000 values, one for each category. What does it look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network seems pretty confident! Lets look at its top 5 guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load labels\n",
    "\n",
    "# Sort top k predictions from softmax output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! Our network knows what it's talking about. Let's have a closer look at what goes on inside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 14: Looking inside the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a convolutional neural network, there's an easy way to visualise the filters learned at the very first layer. We can print each filter to show which colours it reponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a helper function to let you visualise what goes on inside the network\n",
    "\n",
    "def vis_square(weights, padsize=1, padval=0):\n",
    "    #Avoids modifying the network weights\n",
    "    data = np.copy(weights)\n",
    "    \n",
    "    #Normalize the inputs\n",
    "    data -= data.min()\n",
    "    data /= data.max()\n",
    "    \n",
    "    # Lets tile the inputs\n",
    "    # How many inputs per row\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    \n",
    "    # Add padding between inputs\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
    "    \n",
    "    #place the filters on an n by n grid\n",
    "    data = data.reshape((n, n) + data.shape[1:])\n",
    "    \n",
    "    #merge the filters contents onto a single image\n",
    "    data = data.transpose((0, 2, 1, 3) + tuple(range(4, data.ndim)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    \n",
    "    #show the filter\n",
    "    plt.imshow(data)\n",
    "    \n",
    "# Get the weights of the first convolutional layer\n",
    "first_layer_weights = vgg_model.layers[1].get_weights()\n",
    "\n",
    "# first_layer_weights[0] stores the connection weights\n",
    "# first_layer_weights[1] stores the bias weights\n",
    "# For now we're interrested in the connections\n",
    "filters = first_layer_weights[0]\n",
    "\n",
    "# Visualise the filters\n",
    "vis_square(filters.transpose(0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how each filter detects a different property of the input image. Some are designed to respond to certain colours, while some other -- the greyscale looking ones -- detects changes in brightness such as edges. You may notice the brown filter in the top left corner, if we print the values of its weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(filters[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the brown filter we applied to the Grace Hopper portrait above! \n",
    "\n",
    "Another way of visualising the network is to see which neurons get activated as the images traverses the network. A neuron outputing a high value means the pattern it has learnt to detect has been observed. Let's apply this to our kitten image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function fetches the intermediary output from a layer\n",
    "\n",
    "def get_layer_output(model, image, layer):\n",
    "    # This theano function lets us look at the acivations throughout the network\n",
    "    theano_function = theano.function([model.layers[0].input],\n",
    "                                       model.layers[layer].get_output(train=False))\n",
    "    return theano_function(image)[0]\n",
    "\n",
    "layer_output = get_layer_output(vgg_model, img_t, 1)\n",
    "vis_square(layer_output, padsize=5, padval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's worth spending a moment to understand what is going on here. Each pixel in this image is a different neuron in the neural network. Neurons on the same image sample share the same weights and therefore detect the same feature. You can compare the visualised filters above with their corresponding image sample. For example, have find the bright green filter in the original visualisation and look at its corresponding image response.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using this method, it is possible to visualise the deeper parts of the neural network, although they become harder to interpret. You can visualise the output of the second convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the output of the second convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the eighth layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the output of the eighth convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we get further down the network, the representations become smaller in their spatial features thanks to the pooling layers. The final convolutional layers only have dimensions 14 by 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the output of the final convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training your own network\n",
    "\n",
    "Lets train a network! We're going to use the CIFAR10 dataset, in which the goal is to categorise images in one of 10 categories. \n",
    "\n",
    "### Learning Activity 15: Loading the CIFAR10 dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "# Turn our images into floating point numbers\n",
    "\n",
    "# Put our input data in the range 0-1\n",
    "\n",
    "# convert class vectors to binary class matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 16: Building the model\n",
    "\n",
    "Define the model, we will use a small model so it trains faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the model: Our model has six layers, four convolutional, and two fully connected. The first two layers have a\n",
    "# depth of 32, meaning they each detect 32 types of filters. They use 3x3 sized filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz: HOW MANY WEIGHTS IN THE NETWORK?**\n",
    "\n",
    "- How many convolution weights does the first layer contain? What about the second layer?\n",
    "- Are there any other weights in those layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 17: Define the training schedule\n",
    "\n",
    "Using Stochastic gradient descent with an initial learning rate of 0.01 with Nesterov momentum, and a\n",
    "learning rate decay of 1e-6 per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using Stochastic gradient descent with an initial learning rate of 0.01\n",
    "# With Nesterov momentum, and a learning rate decay of 1e-6 per iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 18: Image pre-processing\n",
    "\n",
    "Define the preprocessing of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing, does both normalization and augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,                 # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,                 # set each sample mean to 0\n",
    "        featurewise_std_normalization=True,      # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,      # divide each input by its std\n",
    "        rotation_range=0,                        # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,                   # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,                  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,                    # randomly flip images\n",
    "        vertical_flip=False)                     # randomly flip images\n",
    "\n",
    "# Compute quantities required for featurewise normalization (std, mean)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you're set! You can start training and see the accuracy improve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train, set, go!!! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
