{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks \n",
    "\n",
    "\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "\n",
    "To get an intuitive idea about Neural Networks, let us code an elementary perceptron. In this example we will illustrate some of the concepts we have seen, build a small perceptron and make a link between Perceptron and linear classification.\n",
    "\n",
    "### Learning Activity 1: Generating some data\n",
    "\n",
    "Before working with the MNIST dataset, you'll first test your perceptron implementation on a \"toy\" dataset with just a few data points. This allows you to test your implementations with data you can easily inspect and visualise without getting lost in the complexities of the dataset itself.\n",
    "\n",
    "\n",
    "Start by loading two basic libraries: `matplotlib` for plotting graphs (http://matplotlib.org/contents.html) and `numpy` for numerical computing with vectors, matrices, etc. (http://docs.scipy.org/doc/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let us generate some points in 2-D that will form our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise these points in a scatterplot using the `plot` function from `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the points in a scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, imagine that the purpose is to build a classifier that for a given **new** point will return whether it belongs to the crosses (class 1) or circles (class 0).\n",
    "\n",
    "### Learning Activity 2: Computing the output of a Perceptron\n",
    "\n",
    "Let’s now define a function which returns the output of a Perceptron for a single input point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's build a perceptron for our points\n",
    "\n",
    "def outPerceptron(x,w,b):\n",
    "    innerProd = np.dot(x,w)    # computes the weighted sum of input\n",
    "    output    = 0\n",
    "    if innerProd > b:\n",
    "        output = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s useful to define a function which returns the sequence of outputs of the Perceptron for a sequence\n",
    "of input points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function which returns the sequence of outputs for a sequence of input points\n",
    "\n",
    "def multiOutPerceptron(X,w,b):\n",
    "    nInstances = X.shape[0]\n",
    "    outputs    = np.zeros(nInstances)\n",
    "    for i in range(0,nInstances):\n",
    "        outputs[i] = outPerceptron(X[i,:],w,b)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Activity: Efficient coding of multiOutPerceptron\n",
    "\n",
    "In the above implementation, the simple `outPerceptron` function is called for every single instance. It\n",
    "is cleaner and more efficient to code everything in one function using matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimise the multiOutPerceptron function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the above implementation, the simple `outPerceptron` function is called for every single instance. It is cleaner and more efficient to code everything in one function using matrices.\n",
    "\n",
    "### Learning Activity 4: Playing with weights and thresholds\n",
    "\n",
    "Let’s try some weights and thresholds, and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try some initial weights and thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is clearly not great! it classifies the first point as in one category and all the others in the other one. Let's try something else (an educated guess this time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try an \"educated guess\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better! To obtain these values, we found a **separating hyperplane** (here a line) between the points. The equation of the line is \n",
    "\n",
    "y = 0.5x-0.2\n",
    "\n",
    "\n",
    "**Quiz**\n",
    "- **Can you explain why this line corresponds to the weights and bias we used?**\n",
    "- **Is this separating line unique? what does it mean?**\n",
    "\n",
    "Can you check that the perceptron will indeed classify any point above the red line as a 1 (cross) and every point below as a 0 (circle)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 5: Illustration of the output of the Perceptron and the separating line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the separating line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try adding new points to see how they are classified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add new points and test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the new test points in the graph and plot the separating lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the new points and line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note here that the two sets of parameters classify the squares identically but not the triangle. You can now ask yourself, which one of the two sets of parameters makes more sense? How would you classify that triangle? These type of points are frequent in realistic datasets and the question of how to classify them \"accurately\" is often very hard to answer...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "### Learning Activity 6: Coding a simple gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of a function and it's gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = \\exp(-\\sin(x))x^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f'(x) = -x \\exp(-\\sin(x)) (x\\cos(x)-2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to define python functions which return the value of the function and its gradient at an arbitrary point $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    return np.exp(-np.sin(x))*(x**2)\n",
    "\n",
    "def gradient(x):\n",
    "    return -x*np.exp(-np.sin(x))*(x*np.cos(x)-2) # use wolfram alpha!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the function looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us implement a simple Gradient Descent that uses constant stepsizes. We define two functions, the first one is the most simple version which doesn't store the intermediate steps that are taken. The second one does store the steps which is useful to visualize what is going on and explain some of the typical behaviour of GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simpleGD(x0,stepsize,nsteps):\n",
    "    x    = x0\n",
    "    for k in range(0,nsteps):\n",
    "        x -= stepsize*gradient(x)\n",
    "    return x\n",
    "\n",
    "def simpleGD2(x0,stepsize,nsteps):\n",
    "    x    = np.zeros(nsteps+1)\n",
    "    x[0] = x0\n",
    "    for k in range(0,nsteps):\n",
    "        x[k+1] = x[k]-stepsize*gradient(x[k])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what it looks like. Let's start from $x_0 = 3$, use a (constant) stepsize of $\\delta=0.1$ and let's go for 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try the first given values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple inspection of the figure above shows that that is close enough to the actual true minimum ($x^\\star=0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few standard situations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try the second given values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! so that's still alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try the third given values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not... Visual inspection of the figure above shows that we got stuck in a local optimum.\n",
    "\n",
    "Below we define a simple visualization function to show where the GD algorithm brings us. It can be overlooked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viz(x,a=-10,b=10):\n",
    "    xx  = np.linspace(a,b,100)\n",
    "    yy  = function(xx)\n",
    "    ygd = function(x)\n",
    "    plt.plot(xx,yy)\n",
    "    plt.plot(x,ygd,color='red')\n",
    "    plt.plot(x[0],ygd[0],marker='o',color='green',markersize=10)\n",
    "    plt.plot(x[len(x)-1],ygd[len(x)-1],marker='o',color='red',markersize=10)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the steps that were taken in the various cases that we considered above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the steps taken in the previous cases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise these three cases: \n",
    "- In the first case, we start from a sensible point (not far from the optimal value $x^\\star = 0$ and on a slope that leads directly to it) and we get to a very satisfactory point.\n",
    "- In the second case, we start from a less sensible point (on a slope that does not lead directly to it) and yet the algorithm still gets us to a very satisfactory point.\n",
    "- In the third case, we also start from a bad location but this time the algorithm gets stuck in a local minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacking MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 7: Loading the Python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements for KERAS library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Some generic parameters for the learning process\n",
    "batch_size = 100   # number of instances each noisy gradient will be evaluated upon\n",
    "nb_classes = 10    # 10 classes 0-1-...-9\n",
    "nb_epoch   = 10    # computational budget: 10 passes through the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 8: Loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras does the loading of the data itself and shuffles the data randomly. This is useful since the difficulty\n",
    "of the examples in the dataset is not uniform (the last examples are harder than the first ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also depict a sample from either the training or the test set using the `imshow()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the first image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok the label 5 does indeed seem to correspond to that number!\n",
    "Let's check the dimension of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 9: Reshaping the dataset\n",
    "\n",
    "\n",
    "Each image in MNIST has 28 by 28 pixels, which results in a $28\\times 28$ array. As a next step, and prior to feeding the data into our NN classifier, we needd to flatten each array into a $28\\times 28$=784 dimensional vector. Each component of the vector holds an integer value between 0 (black) and 255 (white), which we need to normalise to the range 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshaping of vectors in a format that works with the way the layers are coded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Remember, it is always good practice to check the dimensionality of your _train_ and _test_ data using the `shape` command prior to constructing any classification model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the dimensionality of train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 60,000 training samples, 10,000 test samples and the dimension of the samples (instances) are 28x28 arrays. We need to reshape these instances as vectors (of 784=28x28 components). For storage efficiency, the values of the components are stored as Uint8, we need to cast that as float32 so that Keras can deal with them. Finally we normalize the values to the 0-1 range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The labels are stored as integer values from 0 to 9. We need to tell Keras that these form the output categories via the function `to_categorical`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set y categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 10: Building a NN classifier\n",
    "\n",
    "\n",
    "A neural network model consists of artificial neurons arranged in a sequence of layers. Each layer receives a vector of inputs and converts these into some output. The interconnection pattern is \"dense\" meaning it is fully connected to the previous layer. Note that the first hidden layer needs to specify the size of the input which amounts to implicitly having an input layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, declare a model with a sequential architecture\n",
    "\n",
    "# Then add a first layer with 500 nodes and 784 inputs (the pixels of the image)\n",
    "\n",
    "# Define the activation function to use on the nodes of that first layer\n",
    "\n",
    "# Second hidden layer with 300 nodes\n",
    "\n",
    "# Output layer with 10 categories (+using softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 11: Training and testing of the model\n",
    "\n",
    "\n",
    "Here we define a somewhat standard optimizer for NN. It is based on Stochastic Gradient Descent with some standard choice for the annealing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definition of the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the right arguments here is non trivial  but the choice suggested here will work well. The only parameter we can explain here is the first one which can be understood as an initial scaling of the gradients. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, launch the learning (fit the model). The `model.fit` function takes all the necessary arguments and trains the model. We describe below what these arguments are:\n",
    "\n",
    "- the training set (points and labels)\n",
    "- global parameters for the learning (batch size and number of epochs)\n",
    "- whether or not we want to show output during the learning\n",
    "- the test set (points and labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Obviously we care far more about the results on the validation set since it is the data that the NN has not used for its training. Good results on the test set means the model is robust.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the results, the accuracy (over the test set) should be in the 98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def whatAmI(img):\n",
    "    score = model.predict(img,batch_size=1,verbose=0)\n",
    "    for s in range(0,10):\n",
    "        print ('Am I a ', s, '? -- score: ', np.around(score[0][s]*100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 1004 # here use anything between 0 and 9999\n",
    "test  = np.reshape(images_train[index,],(1,784))\n",
    "plt.imshow(np.reshape(test,(28,28)), cmap=\"gray\")\n",
    "whatAmI(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it work? (experimental Pt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test  = misc.imread('data/ex7.jpg')\n",
    "test  = np.reshape(test,(1,784))\n",
    "test  = test.astype('float32')\n",
    "test /= 255.\n",
    "plt.imshow(np.reshape(test,(28,28)), cmap=\"gray\")\n",
    "whatAmI(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
